{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate decision tree and visualize it graphically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reload on each run, since we’re editing the decision_tree library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decision_tree import *\n",
    "from graphviz import Digraph\n",
    "from IPython.display import SVG\n",
    "from IPython.core.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample data with a bit of repetition\n",
    "witnessData = {'wit1': ['a', 'b', 'c', 'a', 'd', 'e'],\n",
    "               'wit2': ['a', 'e', 'c', 'd'],\n",
    "               'wit3': ['a', 'd', 'b']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopword list\n",
    "\n",
    "Currently we calculate whether either of the tokens in a skip bigram is a stopword, but we don’t use the information. Since we’re incorporating information about the uniqueness of the skipgrams, we already have an alternative way to find those that contain types that occur frequently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fake stoplist, to ensure that we can identify stopwords and process them last\n",
    "stoplist = {'a', 'c'}  # set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use bitarrays to keep track of which witness tokens have already been placed\n",
    "\n",
    "We also use the bitarrays to count the number of placed tokens, which might be use as part of the scoring process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bitArray_dict is used to keep track of which witness tokens have already been processed\n",
    "bitArray_dict = {k: bitarray(len(witnessData[k])) for k in witnessData}  # create a bitarray the length of each witness\n",
    "for ba in bitArray_dict.values():  # initialize bitarrays to all 0 values\n",
    "    ba.setall(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common skipgram table\n",
    "\n",
    "All skipgrams, with all locations where they occur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csTable: dictionary, in which\n",
    "#   key: two-item tuple representing skipgram normalized token values (token[0], token[1])\n",
    "#   value: list of three-item tuples records all locations where the key occurs: (siglum, offset[0], offset[1])\n",
    "#     In Real Life:\n",
    "#       values will include the t values corresponding to the normalized token values\n",
    "#       use a named tuple or dataclass (https://realpython.com/python-data-classes/)\n",
    "# In this test sample, we find all skip bigrams; in Real Life we would specify parameters for:\n",
    "#   size of skipgram (bi, tri-, etc.; here bi-)\n",
    "#   size of window (maximum distance between first and last members of skipgram; here the full witness length)\n",
    "#   maximum size of skip between members of skipgram (here constrained only by size of window)\n",
    "csTable = collections.defaultdict(list)\n",
    "for key, value in witnessData.items():  # key is siglum, value is list of normalized token readings\n",
    "    # in Real Life the value would also include a non-normalized t property\n",
    "    for first in range(len(value)):  # all first items in bigram\n",
    "        for second in range(first + 1, len(value)):  # pair with all following items\n",
    "            csTable[(value[first], value[second])].append((key, first, second))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shape skipgram table into df and break out features\n",
    "\n",
    "Features are used to short table of (remaining) skipgrams by priority, that is, to determine what to process next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to series before df since list lengths vary\n",
    "csSeries = pd.Series(csTable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert series to dataframe, flatten MultiIndex, label columns\n",
    "csDf = pd.DataFrame(csSeries).reset_index()\n",
    "csDf.columns = [\"first\", \"second\", \"locations\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prioritize skipgrams for processing\n",
    "\n",
    "Three features, in order:\n",
    "\n",
    "1. How many witnesses does a skipgram occur in (depth)? Integer; higher is better\n",
    "1. Does any norm value in a skipgram occur more than once in any witness? Boolean; False is better\n",
    "1. How many times does a skipgram occur in the documents overall? Integer; higher is better, since we’ve already filtered out those where high frequency of a skipgram is accompanied by repetition within a witness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count witnesses for each skipgram (depth of block) and check for uniqueness of skipgram in all witnesses\n",
    "#   extract sigla inside set comprehension to remove duplicates, then count\n",
    "csDf[\"local_witnesses\"] = csDf[\"locations\"].map(lambda x: [location[0] for location in x])\n",
    "csDf[\"unique_witnesses\"] = csDf[\"local_witnesses\"].map(lambda x: set(x))\n",
    "csDf[\"local_witnessCount\"] = csDf[\"local_witnesses\"].str.len()\n",
    "csDf[\"unique_witnessCount\"] = csDf[\"unique_witnesses\"].str.len()\n",
    "csDf[\"witness_uniqueness\"] = csDf[\"local_witnessCount\"] == csDf[\"unique_witnessCount\"]\n",
    "scale = pd.Series([100, 10, 1]) # TODO: check this for polarity\n",
    "csDf[\"priority\"] = pd.np.dot(csDf[[\"unique_witnessCount\", \"witness_uniqueness\", \"local_witnessCount\"]], scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# are both tokens are stopwords? (if so, we’ll process them last)\n",
    "# NB: currently we ignore stopwords\n",
    "csDf[\"stopwords\"] = csDf[[\"first\", \"second\"]].T.isin(stoplist).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort and update row numbers, so that we can traverse the skipgrams as follows\n",
    "csDf.sort_values(by=[\"priority\"], ascending=False, inplace=True)\n",
    "csDf.reset_index(inplace=True, drop=True)  # update row numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csDf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build decision tree\n",
    "\n",
    "Currently we create the root and then expand all branches down three levels. In Real Life:\n",
    "\n",
    "1. Evaluate scores at each stage to decide what to expand and what not to expand.\n",
    "1. Navigate levels with function, instead of in one nested `for` structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root of decision tree inherits empty toList, bitArray_dict with 0 values, and complete, sorted df\n",
    "dtRoot = dtNode([Node(\"#start\"), Node(\"#end\")], \"[none]\", bitArray_dict, csDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtRoot = dtNode([Node(\"#start\"), Node(\"#end\")], \"[none]\", bitArray_dict, csDf)\n",
    "expand_dtNode(dtRoot)\n",
    "for child in dtRoot.children:\n",
    "    expand_dtNode(child)\n",
    "    for grandchild in child.children:\n",
    "        expand_dtNode(grandchild)\n",
    "        for greatgrandchild in grandchild.children:\n",
    "            expand_dtNode(greatgrandchild)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## graphviz Digraph() to be rendered in SVG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = Digraph(format=\"svg\", \n",
    "            graph_attr={\"rankdir\": \"LR\"}, \n",
    "            node_attr={\"fontname\": \"Courier\", \"fontsize\" : \"8\"}) # graphviz digraph for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create functions to add nodes and edges to graphviz Digraph()\n",
    "# node ids are unique integers in string form because \n",
    "#   1. labels may repeat across branches\n",
    "#   2. graphviz ids must be strings\n",
    "\n",
    "def create_adder_n(_g: Digraph, _witnessData: dict): # specify graph and initialize counter when creating adder\n",
    "    _counter = 0 # closure\n",
    "    def add_node(_n: dtNode): # specify only the node when adding it\n",
    "        nonlocal _counter # ensure closure\n",
    "        _placed_tokens = \"%.2f\" % print_placed_witness_tokens(_n) # str(print_placed_witness_tokens(_n))\n",
    "        _counter += 1\n",
    "        if _counter == 1:\n",
    "            _score = \"N/A\"\n",
    "        else:\n",
    "            _score = \"%.2f\" % calculate_score(_n)\n",
    "        _table = str(create_alignment_table(_n.toList, \n",
    "                                            _witnessData,\n",
    "                                            rank_nodes(_n.toList, create_edge_list(_n.toList, _witnessData)), \n",
    "                                            True))\n",
    "        _g.node(str(_counter), label=\"\".join([\"Placed witness tokens (pct): \", _placed_tokens, \n",
    "                                              \"\\nScore (tokens / toList length): \", _score, \n",
    "                                              \"\\n\", _table]))\n",
    "        return _counter # to refer to new node\n",
    "    return add_node\n",
    "\n",
    "# create_alignment_table(_dtNode.toList, _witnessData,\n",
    "#                                 rank_nodes(_dtNode.toList, create_edge_list(_dtNode.toList, _witnessData)),\n",
    "#                                 _print_witness_offset)\n",
    "\n",
    "def create_adder_e(_g: Digraph): # specify graph when creating adder\n",
    "    def add_edge(_u: str, _v: str, _skipgram: str): # networkx in and out nodes for edge\n",
    "        _g.edge(_u, _v, label=_skipgram)\n",
    "    return add_edge\n",
    "\n",
    "a_n = create_adder_n(G, witnessData) # create adder for nodes\n",
    "a_e = create_adder_e(G) # create adder for edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add root node and check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_n(dtRoot) # returns counter as integer; root is 1\n",
    "display(SVG(G.render()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Go down three levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for child in dtRoot.children:\n",
    "    child_node = a_n(child)\n",
    "    a_e(str(1), str(child_node), child.skipgram)\n",
    "    for grandchild in child.children:\n",
    "        grandchild_node = a_n(grandchild)\n",
    "        a_e(str(child_node), str(grandchild_node), grandchild.skipgram)\n",
    "        for greatgrandchild in grandchild.children:\n",
    "            greatgrandchild_node = a_n(greatgrandchild)\n",
    "            a_e(str(grandchild_node), str(greatgrandchild_node), greatgrandchild.skipgram)\n",
    "            for greatgreatgrandchild in greatgrandchild.children:\n",
    "                greatgreatgrandchild_node = a_n(greatgreatgrandchild)\n",
    "                a_e(str(greatgrandchild_node), str(greatgreatgrandchild_node), greatgreatgrandchild.skipgram)\n",
    "display(SVG(G.render()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
